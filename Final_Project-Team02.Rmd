---
title: "IMDB Feature Film Analysis"
author: "T2 Deep Learners: Yue Li, Shuting Cai, Mrunalini Devineni, Siddharth Das"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: united
    highlight: tango
    code_folding: hide
    number_sections: true
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'

---
```{r, echo=FALSE}
knitr::opts_chunk$set(error = TRUE)
```

```{r init, include=FALSE}
knitr::opts_chunk$set(warning = F)
options(scientific=T, digits = 3) 

library(dplyr)
library(ggplot2)
library(ezids)
library(faraway)
library(leaps)
library(tidyr)
library(caret)
library(tree)
library(rpart)
library(regclass)
library(pROC)
library(ROCR)
library(rattle)
library(ResourceSelection)

```

# Introduction

The number of movies released each year has steadily increased from 2000 onward in North America. Being faced with an ever-increasing selection, viewers and critics have developed a range of criteria to review the quality of movies. Social media platforms are rife with fan followings and hate clubs for different movies. We see the effect of these opinions in the movie ratings the film receives. A high movie rating shows how successful and popular the movie is. Movie success is crucial since billions of dollars are invested in making them (Rijul, 2018).

What makes a movie lucrative triggers the interests of researchers worldwide, and there is a lot of research in this regard. Ensemble learning algorithms, like random forest and XGBoost, were used to predict movie rating with social media data and showed the popularity of directors, actors, and writers affected movie rating most (Zahabiya & A.Razia, 2020). Other research indicated duration and budget were deemed more important than the facebook popularity of directors and actors (Sun, 2016). The number of audiences played a vital role in movie rating (Rijul, 2018).

However, previous research has generated inconsistent results on the features that determine a successful movie. Some use data from social media platforms (Facebook, YouTube) that are not reliable for movie reviews. Some of them use small-scaled data covered in recent years.
We aim to predict movie ratings based on movie features using a reliable data set in this project. We are interested in digging deeper into the data to explore which attributes are the most important for a successful movie. This project uses machine learning algorithms to predict movie ratings and provides an investment guide for movie productions and a recommendation platform for audiences to choose high-quality films.

# Data preparation

## Data description

This data set is an IMDb movie extensive data set from Kaggle.
(https://www.kaggle.com/stefanoleone992/imdb-extensive-dataset). IMDb (Internet Movie Databases) is an online database of information related to movies and TV series that include plot summaries, ratings, and reviews. 

There are almost 600,00 movies recorded on IMDb in September 2021. This data set contains four files, movies, names, ratings, and title principles. The movies file comprises 85,855 movie description instances from 1910 to 2020 and 22 attributes, like title name, released year, movie genre, duration, movie language, directors, etc. The rating file contains the weighted average rating, total votes, mean vote, median vote, age group, and gender voting patterns. The names file contains the information of actors, and the title principals file has information about the actor’s role in each movie. The four files have a unique movie title ID and a unique name Id for each person. 

The target variable we are concerned with for this project is the average movie rating. We make use of seven features in our model. Six out of the seven are numeric features: duration, budget, year released, total votes, the average rating of director,  and the weighted average rating of the cast. The movie genre is the only discrete categorical feature. 

## Data Creation and Joining

```{r data_join}
movies <- read.csv('movies.csv', header=TRUE)
drop <- c('usa_gross_income','worlwide_gross_income','metascore','production_company','description','writer', 'language', 'country')
movies <- movies[,!names(movies) %in% drop]
movies <- movies[!(movies$director == "") & !(movies$actors == ""),]
movies$year[movies$year =="TV Movie 2019"] <- "2019"
movies$year <- as.numeric(movies$year)


# movie_ratings <- merge(movies, ratings, by="imdb_title_id" )
movie_ratings <- movies

str(movie_ratings)

title <- read.csv('title_principals.csv', header=TRUE)
title <- title[!(title$category=="director"),]
drop <- c('category','job','characters')
title <- title[,!names(title) %in% drop]
movie_ratings_title <- merge(movie_ratings[,c("imdb_title_id", "date_published", "avg_vote")], title, by="imdb_title_id")
```

# Feature Creation

Feature engineering is when we construct new features from existing data to train a machine learning model. This step is more important than the actual model used since a machine learning algorithm only learns from the data we give it. Hence, creating features relevant to the task is crucial.

Constructing features is very time-consuming because each new feature usually requires several steps, especially when using information from multiple tables. We can group the operations of feature creation into two categories: transformations and aggregations. (Koehrsen, 2018)

A Transformation acts on a single table by creating new features from existing columns. Aggregations are performed across multiple data frames and use a one-to-many relationship to group observations and then calculate statistics. This process involves grouping the main table, calculating the aggregations, and merging the resulting data into the main table. (Koehrsen, 2018)

Now we will talk about the different transformations and aggregations we carried out on our data to obtain meaningful features. 

## Movie Genre

Genre is a category of artistic composition characterized by a particular style, form, or content. In other words, genre categorizes movies. Genre consists of four elements: character, story, plot, and setting. Often movies have genres that overlap, such as adventure in a spy film or crime in a science fiction movie, but one genre is predominant. Film noir, thrillers, and action movies are not genres but a director’s style, but we have included them in our analysis to understand their relevance. Having a movie labeled in a genre assists people in finding a particular movie that they may be interested in watching. Many people like a specific genre or two and will only watch movies in those genres. (Reich, 2017) We have considered the following two methodologies for creating genre features:

```{r genre_selection}
movie_ratings <- movie_ratings %>%
  mutate(
      Romance = grepl('Romance', genre),
      Biography = grepl('Biography', genre),
      Drama = grepl('Drama', genre),
      Adventure = grepl('Adventure', genre),
      History = grepl('History', genre),
      Crime = grepl('Crime', genre),
      Western = grepl('Western', genre),
      Fantasy = grepl('Fantasy', genre),
      Comedy = grepl('Comedy', genre),
      Horror = grepl('Horror', genre),
      Family = grepl('Family', genre),
      Action = grepl('Action', genre),
      Mystery = grepl('Mystery', genre),
      Sci_Fi = grepl('Sci-Fi', genre),
      Animation = grepl('Animation', genre),
      Thriller = grepl('Thriller', genre),
      Musical = grepl('Musical', genre),
      Music = grepl('Music', genre),
      War = grepl('War', genre),
      Film_Noir = grepl('Film-Noir', genre),
      Sport = grepl('Sport', genre),
      Adult = grepl('Adult', genre),
      Documentary = grepl('Documentary', genre),
      Reality_TV = grepl('Reality-TV', genre),
      News = grepl('News', genre)
  )

genre_columns <- c("avg_vote", "Romance", "Biography", "Drama", "Adventure", "History", "Crime", "Western", "Fantasy", "Comedy", "Horror", "Family", "Action", "Mystery", "Sci_Fi", "Animation", "Thriller", "Musical", "Music",  "War", "Film_Noir", "Sport", "Adult", "Documentary", "Reality_TV", "News")

movie_genre_subset <- movie_ratings[,names(movie_ratings) %in% genre_columns]

reg.best25 <- regsubsets(avg_vote~ ., data = movie_genre_subset, nvmax = 15, nbest = 1, method = "exhaustive")
plot(reg.best25, scale = "adjr2", main = "Adjusted R^2")
plot(reg.best25, scale = "r2", main = "R^2")
plot(reg.best25, scale = "bic", main = "BIC")
plot(reg.best25, scale = "Cp", main = "Cp")
# summary(reg.best25)

genre_model_selection = c("Romance", "Biography", "Drama", "History", "Crime", "Horror", "Action", "Mystery", "Sci_Fi", "Animation", "Thriller", "Musical",  "War", "Film_Noir")
genre_formula <- as.formula(paste("avg_vote", paste(genre_model_selection, collapse=" + "), sep="~"))
model.genre <- lm(genre_formula, data = movie_genre_subset)
summary(model.genre)
xkablevif(model.genre, wide=TRUE)

```

Do one-hot encoding for each genre for a movie that belongs to more than one genre. We search for the individual genre pattern in the complete genre string and set a True value for each genre contained in it and a False value for everything else. 

```{r genre2, warning=F}
genre_df1 <- movie_ratings[,c("genre", "avg_vote")]
genre_df2 <- genre_df1 %>%
  separate(genre, c("genre1_",'genre2_','genre3_'),sep =c(', '))

genre_df2$genre1_[is.na(genre_df2$genre1_)] <- "N/A"
genre_df2$genre2_[is.na(genre_df2$genre2_)] <- "N/A"
genre_df2$genre3_[is.na(genre_df2$genre3_)] <- "N/A"
head(genre_df2)

genre_model_selection2 = c("genre1_", "genre2_", "genre3_")
genre_formula <- as.formula(paste("avg_vote", paste(genre_model_selection2, collapse=" + "), sep="~"))
model.genre2 <- lm(genre_formula, data = genre_df2)
summary(model.genre2)
xkablevif(model.genre2, wide=TRUE)

```

We split the genre into three separate sub-genre columns, which will hold each subdivision. We can do this because we know from the data that the genre column will have a maximum of three sub-genres. The movies that do not contain three different genres will coalesce to NA for the later sub-genres. 

We can see from the second methodology that it introduces negligible gains in the R-squared value when compared with the first method. We witness this probably because it also stores the relative importance information of different subgenres. Additionally, it results in high VIF value terms that are ultimately detrimental to model building.  Hence, we choose to build on the first methodology for the sake of controlling the complexity of our final model. 

## Cast Experience

Similarly, cast experience is equally, if not more, vital to the success of a film. When we think of a movie we liked, our minds foremostly go to the artists who acted in these movies. However, gauging the combined experience of the entire cast for every movie seems like a complicated ordeal. Another issue of actors with no prior experience arises, which could mess up the scores. To obtain these values, we utilize two datasets: movies and title principals. 

```{r actor_exp, warning=F}
actor_new <- movie_ratings_title %>%
  group_by(imdb_name_id) %>%
  arrange(date_published) %>%
  mutate(
    actor_exp = 0:(n()-1),
    actor_avg_vote_mean = (cumsum(avg_vote) - avg_vote)/actor_exp
  ) %>%
  ungroup()

actor_new$actor_avg_vote_mean[is.nan(actor_new$actor_avg_vote_mean)] <- 0

actor_df <- actor_new %>%
  group_by(imdb_title_id) %>%
  summarise(
    cast_weighted_avg_rating = sum(actor_exp * actor_avg_vote_mean) / sum(actor_exp)
  )
actor_df <- na.omit(actor_df)
movie_ratings_actorsubset <- merge(movie_ratings, actor_df, by="imdb_title_id")
model.cast <- lm(avg_vote ~ cast_weighted_avg_rating, data = movie_ratings_actorsubset)
summary(model.cast)

```

The methodology we use is as follows:
  
  1) We create a new table at the granularity of one record for each cast member for each movie. 
  
  2) We group by the cast member and repeat the steps as we did for the director to obtain the cast experience and cast mean average vote.
  
  3) We set the cast members without prior experience (NaN values)  with a placeholder value because they will cancel out anyway. 
  
  4) We group the resultant table by the movie Title ID to get back the desired granularity level. 
  
  5) We take the experience of the cast member as a weight to calculate the final mean average vote. More experienced members would contribute more        to the final vote score. 

## Director Experience

Film and TV Directors are some of the most respected individuals in the industry. They are heavily involved in all stages of movie-making: from the very start of the project to the final cut. A director should have a working knowledge of each primary part of making a movie and competently converse with the individuals in charge of those departments. They often get into the field as film editors, actors, or assistants to an established director. We can imagine then how much skills and experience are necessary to create a successful movie. So, we have considered director experience as one of our model input features. However, it cannot be the sole judging factor to decide the movie quality. We should also include the average movie ratings for the films they have directed up to that moment. This approach gives us a complete idea of the importance of the director for movie success. (Film Director, n.d.)

```{r director_exp, waring=F}

movie_ratings_directorsubset <- movie_ratings %>%
  group_by(director) %>%
  arrange(date_published) %>%
  mutate(
    director_exp = 0:(n()-1),
    director_avg_vote_mean = (cumsum(avg_vote) - avg_vote)/director_exp
  ) %>%
  ungroup()

# movie_ratings_directorsubset$director_avg_vote_mean[is.nan(movie_ratings_directorsubset$director_avg_vote_mean)] <- mean(movie_ratings_directorsubset$avg_vote)
movie_ratings_directorsubset <- na.omit(movie_ratings_directorsubset)

# cor_props <- cor(movie_ratings_directorsubset[,5:7], method='pearson')
# corrplot::corrplot(cor_props)

director_model_selection = c("director_avg_vote_mean", "director_exp")
director_year_formula <- as.formula(paste("avg_vote", paste(director_model_selection, collapse=" + "), sep="~"))

model.director <- lm(director_year_formula, data = movie_ratings_directorsubset)
summary(model.director)
xkablevif(model.director)
```

The methodology we use is as follows:
  
  1) We group the movies by director and arrange them in increasing order of the published year.
  
  2) We set the director experience in incremental order starting from zero for their first project.
  
  3) We take a cumulative sum of the movie ratings for the past movies and divide it by the director experience.
  
  4) We omit the records where the director does not have prior experience as we do not want to introduce bias into our model by imputation. 

## Numeric Columns

```{r budget_cleaning, include=TRUE, warning=F}

movie_ratings_budget_subset <- movie_ratings[!(movie_ratings$budget == ""),]
movie_ratings_budget_subset <- separate(movie_ratings_budget_subset, budget, c("symbol", "budget_num"), sep=" ")
movie_ratings_budget_subset$budget_num <- as.numeric(movie_ratings_budget_subset$budget_num)
currency <- data.frame(symbol=c('$', 'ITL', 'ROL', 'SEK', 'FRF', 'NOK', 'GBP', 'DEM', 'PTE', 'FIM', 'CAD', 'INR', 'CHF', 'ESP', 'JPY', 'DKK', 'NLG', 'PLN', 'RUR', 'AUD', 'KRW', 'BEF', 'XAU', 'HKD', 'NZD', 'CNY', 'EUR', 'PYG', 'ISK', 'IEP', 'TRL', 'HRK', 'SIT', 'PHP', 'HUF', 'DOP', 'JMD', 'CZK', 'SGD', 'BRL', 'BDT', 'ATS', 'BND', 'EGP', 'THB', 'GRD', 'ZAR', 'NPR', 'IDR', 'PKR', 'MXN', 'BGL', 'EEK', 'YUM', 'MYR', 'IRR', 'CLP', 'SKK', 'LTL', 'TWD', 'MTL', 'LVL', 'COP', 'ARS', 'UAH', 'RON', 'ALL', 'NGN', 'ILS', 'VEB', 'VND', 'TTD', 'JOD', 'LKR', 'GEL', 'MNT', 'AZM', 'AMD', 'AED'), currency=c(1, 0.0005828, 0.23, 0.11, 0.171704, 0.11, 1.32, 0.57618851, 0.00562705, 0.189272, 0.79, 0.013, 1.08, 1.13, 0.0088, 0.15, 0.51043, 0.25, 0.014, 0.71, 0.00085, 0.0279869, 1786.67, 0.13, 0.68, 0.16, 1.13, 0.00015, 0.0077, 1.4307619, 0.074, 0.15, 0.00468946, 0.02, 0.0031, 0.018, 0.0065, 0.044, 0.73, 0.18, 0.012, 0.082081, 0.73, 0.064, 0.03, 0.00331346, 0.063, 0.0083, 0.00007, 0.0057, 0.048, 0.58, 0.071921, 0.0767545, 0.24, 0.000024, 0.0012, 0.037403675, 0.32635053, 0.036, 2.62886, 1.60544, 0.00025, 0.0099, 0.037, 0.23, 0.0093, 0.0024, 0.32, 0.00000000215874, 0.000043, 0.15, 1.41, 0.0049, 0.32, 0.00035, 0.59, 0.002, 0.27))

movie_ratings_budget_subset <- merge(movie_ratings_budget_subset, currency, by='symbol')
movie_ratings_budget_subset$budget_num <- movie_ratings_budget_subset$budget_num * movie_ratings_budget_subset$currency
movie_ratings_budget_subset <- movie_ratings_budget_subset[!(movie_ratings_budget_subset$budget_num == 0),]

# Normalization
movie_ratings_budget_subset$budget_num <- log(movie_ratings_budget_subset$budget_num)
movie_ratings_budget_subset$votes <- log(movie_ratings_budget_subset$votes)
movie_ratings_budget_subset$reviews_from_users <- movie_ratings_budget_subset$reviews_from_users/100
movie_ratings_budget_subset$reviews_from_critics <- movie_ratings_budget_subset$reviews_from_critics/100

numeric_columns = c("budget_num", "year", "duration", "votes", "reviews_from_users", "reviews_from_critics")
numeric_formula <- as.formula(paste("avg_vote", paste(numeric_columns, collapse=" + "), sep="~"))

model.numeric <- lm(numeric_formula, data = movie_ratings_budget_subset)
summary(model.numeric)
xkablevif(model.numeric)

```

Our data set consists of several numeric columns, although we did not use most of them. We have altered the essential numeric columns in the below-mentioned ways. 

1) We recast the year of release as a numeric column. We tried splitting the column into relevant decades since the 1960s, the 1970s, and the 1980s   have their unique flavor, but they did not give us significant gains for our R-squared scores, so we resolved to keep our model simplistic          instead.

2) The movie budget can be a good indicator of how well it would do at the box office. Since our IMDB data set houses movie information from around the globe, the budget information is available in different currencies. In the data set, aside from US dollars, more than 60 monetary features from other countries existed. We transformed these features into US dollars. We had a choice to consider only those movies made in US dollars, or we could try modifying the data to bring them all to the same scale using conversion rates. Another problem we faced was the issue of inflation and variations in currency exchange rates. For the sake of simplicity, we chose to ignore the effect of inflation on our budget numbers and only considered converting the values according to the exchange rates at the time of building the project. Hence, we hard coded the values to estimate the scale between movie budgets across different countries.

3) We applied the log transformations for the movie budget and votes columns because they generally store massive values. Inputting these values to the model would skew our results, so we decided to control their values. Additionally, we only needed the order of the values (hundred-thousands, millions, billions, etc.) to understand the scale of the movie or movie votes.

4) Reviews from users and critics were individual values against each movie out of a hundred, so we scaled them to be between 0 and 1. 

Three categorical variables needed more work compared with numeric ones. It not only required reasonable logic but also corresponding programming skills.


## Final dataset creation

```{r final_dataset, include=TRUE}
genre_columns <- c("imdb_title_id", "avg_vote", "Romance", "Biography", "Drama", "Adventure", "History", "Crime", "Western", "Fantasy", "Comedy", "Horror", "Family", "Action", "Mystery", "Sci_Fi", "Animation", "Thriller", "Musical", "Music",  "War", "Film_Noir", "Sport", "Adult", "Documentary", "Reality_TV", "News")
director_columns = c("director_avg_vote_mean", "director_exp", "imdb_title_id")
numeric_columns = c("budget_num", "year", "duration", "votes", "reviews_from_users", "reviews_from_critics", "imdb_title_id")
movie_ratings_final1 <- merge(movie_ratings[,genre_columns], movie_ratings_actorsubset[,c("cast_weighted_avg_rating", "imdb_title_id")], by="imdb_title_id")
movie_ratings_final2 <- merge(movie_ratings_final1, movie_ratings_directorsubset[,director_columns], by="imdb_title_id")
movie_ratings_final3 <- merge(movie_ratings_final2, movie_ratings_budget_subset[,numeric_columns], by="imdb_title_id")
```

We use an inner join on all the feature columns created so that all the records that did not get dropped in any pre-processing steps result in a final intersection that is ready for modeling. 

# Model Creation

For this project, our SMART questions were to predict movie ratings with a regression model and recommend good movies with a classification model. Therefore, we chose the linear regression model for the first question and the logistic regression model for the second question. After finishing the linear regression model, we gained feature importance by building a decision tree model.

## Linear Regression

Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is an explanatory variable, and the other is a dependent variable. 
Before attempting to fit a linear model to observed data, a modeler should first determine whether there is a relationship between the variables of interest. Doing this does not necessarily imply that one variable causes the variance in the other variable, but there is some significant association between the two variables. A linear regression line has an equation form of Y = a + bX, where X is the explanatory variable and Y is the dependent variable. The line slope is b, and a is the intercept (the value of y when x = 0). (Prabhakaran, 2016)

```{r Linear Regression, warning=FALSE}
genre_columns <- c("Romance", "Biography", "Drama", "Horror", "Action", "Sci_Fi", "Animation", "War", "Film_Noir")
cast_columns = c("director_avg_vote_mean", "director_exp", "cast_weighted_avg_rating")
# numeric_columns = c("budget_num", "year", "duration", "votes", "reviews_from_users", "reviews_from_critics")
numeric_columns = c("budget_num", "year", "duration", "votes")
total_columns = paste(paste(numeric_columns, collapse=" + ") , paste(cast_columns, collapse=" + "), paste(genre_columns, collapse=" + "), sep = " + ")
complete_formula <- as.formula(paste("avg_vote", total_columns, sep=" ~ "))

train_idx <- createDataPartition(movie_ratings_final3$avg_vote, p=0.7, list=FALSE)
data_train <- movie_ratings_final3[train_idx,]
data_test <- movie_ratings_final3[-train_idx,]

model.final <- lm(complete_formula, data = data_train)
summary(model.final)
xkablevif(model.final)

predict_test <- predict(model.final, data_test, type='response')

test.pred <- predict(model.final, data_test)
test.y    <- data_test$avg_vote

SS.total      <- sum((test.y - mean(test.y))^2)
SS.residual   <- sum((test.y - test.pred)^2)
SS.regression <- sum((test.pred - mean(test.y))^2)
test.rsq <- 1 - SS.residual/SS.total  
paste("Test R-squared value is ", test.rsq)
```

In our project, we build a linear model using specific features that are:
  
  1) Genre column subset
  
  2) Director average vote mean
  
  3) Director experience
  
  4) Cast weighted average rating
  
  5) Budget number
  
  6) Year
  
  7) Duration
  
  8) Total Votes

We split the records into 70% training and 30% testing data, ran the model on train data, predicted the outcomes for the test data, and finally compared the R-squared values in both cases. 

We obtained a training R-squared value of `r summary(model.final)$r.squared` and a testing r-squared value of `r test.rsq`.
Though the accuracy is low, this model has a lesser residual error. Using this technique, we proposed a way to recommend higher-rated movies.

## Decision Tree

Decision Trees are versatile machine learning algorithms that can perform classification and regression tasks. They are helpful techniques capable of fitting complex data sets. Besides, decision trees are fundamental components of random forests, which are among the most effective Machine Learning algorithms available today. 

Feature importance refers to techniques that assign a score to input features based on their usefulness in predicting a target variable. There are many types and sources of feature importance scores, the one we used to be decision tree. Decision tree algorithms offer importance scores based on criterion reduction to select split points, like Gini or entropy. We can use the RPART library to train the tree model and obtain the variable importance in its summary. (Donges, 2021) We used a decision tree regressor to find the best features for prediction, and the results are as follows:

```{r Decision Tree, include=TRUE, fig.height=10, fig.width=10}

model.tree <- rpart(complete_formula, data = movie_ratings_final3, control = list(maxdepth = 7, cp=0.005))
# summary(model.tree)
plot(model.tree)
text(model.tree, cex=0.8)
fancyRpartPlot(model.tree)
feature_importances <- data.frame(model.tree$variable.importance)
feature_importances$variables <- row.names(feature_importances)
names(feature_importances) <- c("importances", "variables")
feature_importances <- feature_importances[order(feature_importances$importances),]

ggplot(aes(x=variables, y=importances), data=feature_importances) + 
  geom_bar(stat='identity') + 
    coord_flip()  

```

We have constructed graphs here using two methods: one from the base R plot function and the other using fancyRPartPlot. We see how the root node is split based on cast_weighted_avg_rating and then on director_avg_vote_mean. It shows us that these two variables are the most important, further confirmed by the feature importance bar graph. We tried to get the output in decreasing order of importance but were strangely unable to do so using GGPlot. We find the other essential features to be total votes, budget, genre, year, etc. 

## Logistic Regression

### Target Variable

We built the logistic model with the target variable of the average vote, using 0.5 of quantile to split the average movie vote and encoding them. We found the 50th percentile of the movie rating to be 6.3. We encode the target variable with one if it is greater than that value (good) and encode it with zero if it is lesser than that value (not good).

```{r logistic Regression, include=TRUE, warning=FALSE}
## Avg_vote split

movie_ratings_final3$avg_vote[which(movie_ratings_final3$avg_vote <=(quantile(movie_ratings_final3$avg_vote, probs = 0.5)))] = 0
movie_ratings_final3$avg_vote[which(movie_ratings_final3$avg_vote >(quantile(movie_ratings_final3$avg_vote, probs = 0.5)))] = 1

```

### Train test splitting

Let us split the data into training and test set, so that we can estimate test errors. The split of train and test with 7:3 ratio will be used here.

```{r train&test, warning=FALSE }

## train and test split
train <- createDataPartition(movie_ratings_final3$avg_vote,p=0.7,list=FALSE)
data_train <- movie_ratings_final3[train,]
data_test <- movie_ratings_final3[-train,]

```

### Model fitting

We have built a logistic model to predict the goodness of a movie with the features of movie genres, movie director experience, movie casting, the movie release year, movie duration, movie votes from the audience, and movie budget.
We train the logistic model with the categorical features of the genre, director, the actor, and numerical ones of the year, duration, budget, and votes. We have used (regsubsets) to select the subset of genres effective in predicting the average rating. They are Romance, Biography, Drama, Horror, Action, Sci_Fi, Animation, War, and Film Noir. We use the training data set for building the model and then predict how good the film is on the test data set model. Finally, we check the accuracy of the model. 

```{r logitmodel, warning=FALSE}

model_1 <- glm(complete_formula, data = data_train, family = "binomial" )

```

We can see the summary of the logit model here: 

```{r summary of log model, warning=FALSE }

summary(model_1)

```


```{r , warning=FALSE}

xkabledply(model_1, title = paste("Logistic Regression :") )

# coeff

coeff = coef(model_1)
xkabledply( as.table(coeff), title = "Coefficients in Logit Reg" )

```

All the coefficients are found significant because of small p-values. The movie released year, director experience, and the movie genres of Romance, Horror, Action and Sci-Fi have negative effects on the goodness of movie with (avg_vote= 1) while the rest of features have the positive effects on the goodness of movie with (avg_vote= 1).

### Confidence Intervals

We can determine the confidence intervals of each coefficient:

```{r ConfInt, results='markup', collapse=F, warning=FALSE}

xkabledply(confint(model_1), title = "CIs using profiled log-likelihood" )

```

### Model evaluation

After building the model, we do the model evaluation to decide whether the model performs better. It is critical to consider the model outcomes according to every possible evaluation method. We apply different techniques that can provide different perspectives to evaluate the model.

#### Analysis of deviance

We can quickly run a chi-squared test. The chi-square value is based on the ability to predict the target variable with and without the independent variables. 

```{r Chi-squared test, warning=FALSE}

anova(model_1,test="Chisq")

```

The difference between the null and the residual deviances shows us how well our model is doing compared to the null model (model with only the intercept). We want this gap to be as big as possible. Analyzing the table, we can see the drop in deviance when adding each variable one at a time. Again, adding the numerical variables of budget, year, duration, votes reduces the residual deviance. The other variables seem to improve the model less even though the genre of Romance, Biography, Drama, Horror, and Action have a low p-value. A high p-value here indicates that the model without the variable explains more or less the same amount of variation. 

#### Confusion matrix 

We use a confusion matrix to describe the performance of a classification model on a set of test data for which the actual values are known.

```{r confusion matrix, warning=FALSE}

xkabledply(confusion_matrix(model_1))
precision = confusion_matrix(model_1)[5] / (confusion_matrix(model_1)[5] + confusion_matrix(model_1)[4])
recall = confusion_matrix(model_1)[5] / (confusion_matrix(model_1)[5] + confusion_matrix(model_1)[2])
f1_score = (2*recall*precision) / (recall + precision)

```

We can obtain the value of the F1 score, precision, and recall score from the confusion matrix. These provide better insights into the prediction as compared to accuracy performance metrics. We obtain an F1 score of `r f1_score`. We also get a precision value of `r precision` and a recall value of `r recall`. 

#### Hosmer and Lemeshow test  

The Hosmer and Lemeshow Goodness of Fit test can be used to evaluate logistic regression fit. 

```{r HosmerLemeshow, warning=FALSE}

ratingLogitHoslem = hoslem.test(data_train$avg_vote, fitted(model_1)) 
ratingLogitHoslem
```

The Hosmer-Lemeshow test is a goodness-of-fit test for logistic regression, especially risk prediction models. A goodness of fit test tells you how well your data fits the model. Small p-values mean that the model is a poor fit. Like most goodness of fit tests, these small p-values (usually under 5%) mean that your model is not a good fit. But large p-values don’t necessarily mean that your model is a good fit, just that there isn’t enough evidence to say it’s a poor fit. The p-value of `r ratingLogitHoslem$p.value` is relatively small. It indicates the model is not a good fit in the training data set.

#### ROC-AUC

Receiver-Operator-Characteristic (ROC) curve and Area-Under-Curve (AUC) measure the True positive rate (or sensitivity) against the False positive rate (or specificity). The area-under-curve is always between 0.5 and 1. Values higher than 0.8 are considered a good model fit.  

```{r roc_auc_ data train, warning=FALSE}

prob=predict(model_1, type = "response" )
data_train$prob=prob
h <- roc(avg_vote~prob, data=data_train)
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)

```

As a rule of thumb, a model with good predictive ability should have an AUC closer to 1, and our value of the area-under-curve is `r auc(h)` is more than 0.8. This test against the Hosmer and Lemeshow test shows that the model is considered a good fit. 

```{r roc_auc_ data test, warning=FALSE}

p <- predict(model_1, data_test, type="response")
pr <- prediction(p, data_test$avg_vote)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc

```

The area under the ROC curve (AUC) is a popular measure of the accuracy of a diagnostic test. In general, higher AUC values indicate better test performance. We have an AUC of `r auc(h)` for our testing data set. Since this value is close to 1, we can safely say that the model is performing well. 

#### McFadden  

McFadden is another evaluation tool we can use on logit regressions. It is part of what is called pseudo-R-squared values for evaluation tests. We can calculate the value directly from its definition.

```{r McFadden_direct, warning=FALSE}
ratingNullLogit <- glm(avg_vote ~ 1, data = data_train, family = "binomial")
mcFadden = 1 - logLik(model_1)/logLik(ratingNullLogit)
mcFadden
```

Analogous to the coefficient of determination R-squared, the McFadden value of `r mcFadden[1]` means that the model explains only `r mcFadden[1] * 100`% of the variations in y by the predictor variables in the model. The pseudo-R-squared value between 0.2 to 0.4 indicates an excellent fit.


#### Train and Test Accuarcy
```{r acc, warning=FALSE}
## train and test accuarcy
fitted.results <- predict(model_1,data_train,type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)
misClasificError <- mean(fitted.results != data_train$avg_vote)
train_acc = 1-misClasificError

test.results <- predict(model_1,data_test,type='response')
test.results <- ifelse(test.results > 0.5,1,0)
misClasificError_test <- mean(test.results != data_test$avg_vote)
test_acc = 1-misClasificError_test

```

The accuracy of the train data set is `r train_acc`, and the accuracy of the test data set is `r test_acc`. With the logistic model we built, we have `r test_acc*100`% accuracy while predicting the goodness of the movies from the test data set.  

# Conclusions

Based on the massive movie information, it was interesting to analyze and discover the factors that make a movie more successful than others. We learned what kind of movies are more successful, in other words, get higher IMDB ratings. We also show the results of this analysis intuitively by visualizing some of the outcomes in R.
Further, we considered the IMDB rating a response variable and focused on making predictions by analyzing the rest of the variables in the IMDB movie data set. We understand the relative importance of different variables in predicting the success of a movie. The results help film companies to understand the secret of generating a commercially successful film.

Accuracy table for the different models:

+-------------------+------------------------+------------------------------------------+
|Dataset            | Logistic Regression    | Linear Regression                        |
+-------------------+------------------------+------------------------------------------+
|Training           | `r train_acc*100`%	               | `r summary(model.final)$r.squared*100`%  |
+-------------------+------------------------+------------------------------------------+
|Testing	          | `r test_acc*100`%	               | `r test.rsq*100`%                        |
+-------------------+------------------------+------------------------------------------+

# References

José Gabriel Navarro. (2021, November 17). 
*Number of movies released in the United States and Canada from 2000 to 2020*. https://www.statista.com/statistics/187122/movie-releases-in-north-america-since-2001/

Zahabiya M., A.Razia S.,& Sujala D. (2020, November 8). Movie Rating Prediction using Ensemble Algorithms.
*International Journal of Advanced Computer Science and Applications, Vol. 11, No. 8*. https://thesai.org/Downloads/Volume11No8/Paper_49-Movie_Rating_Prediction.pdf

Chuan Sun. (2016, August, 22).
*Predict Movie Rating*. https://nycdatascience.com/blog/student-works/web-scraping/movie-rating-prediction/

Will Koehrsen. (2018, June 2).
*Automated Feature Engineering in Python*. https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219

John Reich. (2017, July 11).
*What Is Genre and How Is It Determined?*. https://milnepublishing.geneseo.edu/exploring-movie-construction-and-production/chapter/2-what-is-genre-and-how-is-it-determined/
In *Exploring Movie Construction and Production*. (Part 1: Construction, Chapter 2). Open SUNY Textbooks.

Film Director. (n.d.). 
*Academic Earth*. Retrieved December 15, 2021 from https://academicearth.org/careers/film-director/

Niklas Donges. (2021, July 22).
*A Complete Guide to the Random Forest Algorithm*. https://builtin.com/data-science/random-forest-algorithm

Selva Prabhakaran. (2016, n.d.).
*Logistic Regression*. http://r-statistics.co/Logistic-Regression-With-R.html
